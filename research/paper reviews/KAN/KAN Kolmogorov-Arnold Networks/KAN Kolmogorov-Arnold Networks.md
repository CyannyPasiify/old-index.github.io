---
title: "[Review] KAN: Kolmogorov–Arnold Networks"
description: A new parameterization of neurons
---

<!--KaTeX-->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">

  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script>
  <script>
      document.addEventListener("DOMContentLoaded", function() {
          renderMathInElement(document.body, {
              // ...options...
          });
      });
  </script>

# [Review] KAN: Kolmogorov–Arnold Networks

一个全新的神经元参数化方案，基于Kolmogorov–Arnold定理的B样条非线性参数化。

## 收录

arXiv preprint

Liu Z, Wang Y, Vaidya S, et al. Kan: Kolmogorov-arnold networks[J]. arXiv preprint arXiv:2404.19756, 2024.

[[source]](https://doi.org/10.48550/arXiv.2404.19756)

## Kolmogorov-Arnold定理

论文主要的理论依据在于Kolmogorov-Arnold定理。这个定理是由苏联数学家安德烈·科尔莫戈洛夫（Andrey Kolmogorov）首先提出，并由他的学生弗拉基米尔·阿诺尔德（Vladimir Arnold）在1957年进一步发展。定理最初的动机是探讨**多元函数可以如何被一组更简单的函数表示**。

Kolmogorov-Arnold定理指的是，对于任何定义在闭区间上的连续函数$$ f\left(x_1, ..., x_n\right) $$ ，存在一系列的一维连续函数，使得该多变量函数可以表示为：

$$
f\left(x_1, ..., x_n\right)=\sum_{q=1}^{2n+1}\Phi_q\left(\sum_{p=1}^{n}\phi_{q,p}\left(x_p\right)\right)
$$

其中，

l $$ \phi_{q,p}$$是将单个变量映射到实数的一维连续函数。

l $$ \Phi_q$$是将实数映射到实数的一维连续函数，它处理由$$ \phi_{q,p}$$映射后的和。

这个定理表明**多维函数可以通过一系列的一维函数和简单的加法来构建**。这种结构**简化了多维函数的复杂性**，因为一维函数相对来说更易于处理和学习。从机器学习和神经网络的角度看，这提供了一种潜在的网络架构设计思路，即通过学习一系列的一维函数来近似复杂的多维函数。Kolmogorov-Arnold定理提供的公式形式可以视作一个2层网络，它们分别有n、2n+1维输入和2n+1、1维输出。论文给出了这个2层网络向任意层网络的推广，即KAN。

## 与MLP比较

MLP是当前最主流的深度神经网络（DNN）形式。它由多个层组成，每层都包含多个神经元，这些神经元通过**激活函数**进行非线性变换。

MLP的理论依据是**通用近似定理（Universal Approximation Theorem，UAT）**，它确立了MLP的优化范式，即足够层数的MLP能够无限近似任意目标函数。定理的主要内容是**对于任何在闭区间上定义的连续函数和任意的误差$$ \epsilon$$，都存在一个具有至少一个隐藏层的前馈神经网络，可以找到一组权重和偏置，使得该神经网络的输出函数在整个定义域内与目标函数的最大偏差小于$$ \epsilon$$**。

这个定理解释了为什么神经网络可以被应用于各种复杂的模式识别和非线性回归任务，但在实际应用中，如何设计网络结构、选择合适的激活函数和训练算法以达到所需的逼近精度仍然是重要的研究和工程问题。此外，UAT定理并没有涉及网络训练的收敛速度或是所需的训练数据量，因此它也有着它的局限性。

![Compare MLP KAN](Assets/Compare%20MLP%20KAN.png)

上图是论文中对MLP和KAN的对比。与MLP类似，KAN实际上也具有全连接结构，区别在于，MLP在节点（神经元）上放置固定的激活函数，而KAN在边缘（权重）上放置可学习的激活函数。因此，Kolmogorov-Arnold网络完全没有线性权重矩阵：**每个权重参数都由一种可学习的一维样条函数替代**。Kolmogorov-Arnold网络的节点仅对输入信号进行求和，不进行任何非线性处理，因此在节点只需要简单的加法。而且KAN通常允许比MLP更小的计算图，论文中以PDE求解为例进行了说明：1个2层宽度为10的KAN比一个4层宽度为100的MLP精度高100倍且参数效率高100倍。

论文主要贡献不只是使用Kolmogorov-Arnold表示定理来构建神经网络，而是将原始的Kolmogorov-Arnold**推广到任意宽度和深度**。KAN的实质可以理解成样条和MLP的组合，那么结合后可以得到他们各自的优势。

首先来看看**样条函数(Spline function)**，它是一种特别的数学函数，用于在数值分析、插值、和曲线拟合中实现平滑曲线的生成。样条函数通常由一系列低阶多项式分段定义，这些多项式在分段的接合点上连续并具有连续的导数。这些多项式的阶数（degree）决定了样条的光滑程度，常见的如线性样条（一阶）、二次样条（二阶）、和三次样条（三阶）。其中，三次样条是最常用的类型，因为它在提供足够平滑的曲线的同时，计算复杂度和拟合效果之间达到了良好的平衡。样条函数因其数学性质和灵活性，**在低维空间中的数据插值和函数逼近中表现出色**。然而，当应用到高维空间时，样条函数面临一系列挑战，主要是由于**维数灾难**（Curse Of Dimensionality，COD）引起的。随着数据的维度增加，为了维持样条插值的准确性和平滑性，所需的数据量和计算复杂度会呈指数级增长。

而对于MLP，它能够通过特征学习来挖掘数据的复合结构，每一层的神经元可以从前一层学到的特征中进一步抽取信息，这种分层学习的方式使得MLP在高维数据中能够捕捉更深层次的、非线性的数据结构和关联，从而缓解维数增加带来的影响，从而使其在维数灾难中表现得更为鲁棒。

但MLP在低维数据上的表现不如样条函数精确，且在模拟某些特定函数（如指数和正弦函数）时可能效率低下，主要有以下原因：

**1. 非线性激活函数的限制**

MLP通常使用非线性激活函数（如sigmoid、tanh或ReLU）来使网络具备学习复杂非线性关系的能力。然而，这些标准的非线性激活函数**可能并不直接适合精确模拟一些特定的数学函数**，如指数函数或三角函数。要通过这些基本的非线性函数来精确拟合指数或正弦曲线，可能需要大量的神经元和复杂的网络结构，这会导致学习过程中的效率问题。

**2. 维数灾难的反面影响**

虽然MLP设计初衷是为了解决高维数据问题，但这种设计在低维数据上可能造成资源浪费和效率低下。例如，一个为处理成百上千维度数据而设计的复杂MLP在仅有一两个输入特征的任务上可能**表现过剩且运算效率不高**。

那么结合样条函数和MLP的优势而得到的KAN，它能够学习数据的复合结构（外部自由度）和优化一元函数（内部自由度），这使它们能够精确地学习和优化特征，从而在复杂性能和精确度上超越单纯的MLP或样条函数。

KAN内部使用样条函数来建模数据，外部则利用类似MLP的结构来优化和调整这些样条函数学习到的特征，因此在性能上优于MLP。

## KAN架构

在KAN中，每个连接不再是简单的权重，而是一个可学习的一维函数。这些函数通常通过样条或其他平滑函数进行参数化，使得网络可以学习在每个维度上最适合数据的非线性变换。每个输入变量$$ x_i$$通过一组独立的一维函数$$ \phi_{p,q}\left(x_i\right)$$处理，其中p表示输入变量的索引，q表示输出维度的索引。

组合函数在网络高层使用，它们将一维函数的输出进行组合和加工。这些函数也可以通过一维样条或其他形式进行参数化，使得输出不仅仅是简单的线性组合，而是能够捕捉输入间复杂的交互效应。

KAN层的公式表达为

$$
x_{l+1,j}=\sum_{i=1}^{n_l}x_{l,i,j}=\sum_{i=1}^{n_l}{\phi_{j,i}\left(x_{l,i}\right)},j=1,...,n_{l+1}
$$

它描述了如何在第l层和第l+1层之间进行计算，即下一层的每个节点的激活值是当前层所有节点输出通过相应的一维函数处理后的总和。

论文提出了3个优化技巧：

**1. 残差激活函数**：每个激活函数$$ \phi(x)$$被设计为残差形式，即包括一个固定基函数$$ b(x)$$和一个带有可学习参数的B样条函数。$$ b(x)$$通常选择为SiLU函数，提供了非线性的基本变换，而B样条部分允许进一步的细粒度调整。这种组合提高了模型对非线性关系的适应性和表达力。

**2. 激活函数的尺度初始化**：为了确保网络在初始化阶段具有合理的行为，激活函数的部分，特别是B样条部分，通常初始化为接近零的小值，而权重$$ w$$使用Xavier初始化。这种初始化方法帮助避免训练初期的梯度爆炸或消失问题，促进了更稳定的学习过程。

**3. 动态更新样条网格**：由于B样条是定义在有限区间的，但激活值可能在训练过程中超出这些区间，因此提出了动态更新网格的方法。这意味着根据激活函数输入的实际分布，调整定义样条函数的网格点，确保模型能够适应输入数据的变化，维持优化性能。

尽管参数数量在理论上可能看起来比传统MLP模型更多，但论文中用实验表明，由于KAN通常可以用更少的层宽度N来达到与更宽的MLP相当或更好的性能，所以在实际应用中它们通常更加参数高效。这种高效性不仅可以节省存储空间，还可以改善泛化能力，因为较少的参数数量有助于减少过拟合的风险。

### KAN的近似能力和神经缩放定律

与MLP的理论基础UAT类似，KAN在理论上能够**以任意精度近似任何连续函数**，只要网络有足够的层和每层有足够的节点。实际中，这意味着通过增加网络的深度和宽度，可以提高近似的精度。此外，KAN的**每个节点实现的是一维函数变换**，这相比传统的神经网络在处理高维数据时更为高效和有效。

如果函数可以通过KAN表示，那么存在一组特定的B样条函数，这些样条函数可以用来以任意小的误差近似原函数。这种近似的质量取决于B样条函数的网格大小，网格越细，近似的精度越高。论文中称此定理为KAT近似理论。这证明了KAN在处理复杂和高维数据时的潜在优势，因为可以通过细化网格来逐步提高模型的表现，而无需从头开始重建模型（但MLP缩放后却需要重新训练）。

### 可解释性：KAN与人交互性

为使KAN具有更高的可解释性，论文也提出了一些简化技术：

**稀疏化（Sparsification）**

对KAN中的激活函数进行稀疏化处理，主要通过L1正则化来实现。由于KAN中没有传统的线性权重，而是使用一维函数，论文使用函数在多个采样点的平函数均值作为函数的L1范数，并追加一个熵正则化项。L1正则指导KAN在L1范数上接近原函数，熵正则指导KAN尽量学习具有价值的信息（熵越大，说明信息量越低，越接近随机，这是我们不希望的）。

**可视化（Visualization）**

通过将激活函数的重要性可视化，可以帮助研究者和开发者更好地理解模型的工作原理，甚至发现新的数学定理。可视化KAN时根据激活函数参数的tanh激活值设置边的透明度来反映其相对重要性。

![KAN visualization and pruning](Assets/KAN%20visualization%20and%20pruning.png)

**剪枝（Pruning）**

在训练过程中或训练后对模型进行剪枝，移除不重要的节点或激活函数，从而减少模型的复杂性。剪枝后的模型通常更加高效，且更易于分析和部署，可以据其写出主要规律模式的表达式。

**符号化（Symbolification）**

通过可视化与剪枝，研究者可能从其中发现了一些数学分析信息，例如周期性、凹凸性质等，KAN允许主动调用接口将一些激活函数参数化为特定的数学函数（如正弦、指数等）并带有伸缩、偏置系数等可学习参数，而不是完全依赖数据驱动的方式。这种方法有助于提升模型的数学解释性，使其输出更加可预测。

通过使模型简化并增加交互性，KAN不仅可以在复杂数据集上表现出色，还可以更好地与用户沟通，使其决策过程透明化。这些改进对于提升模型的实用性和可信度至关重要。

## KAN相关实验

论文的后续部分主要是为论证KAN的准确性和可解释性而开展了多种任务的实验。准确性方面，通过一系列数据拟合和偏微分方程（PDE）求解的实验，展示了KAN在这些任务上相比传统多层感知机（MLP）有更高的准确性。实验结果表明，即使在网络规模较小的情况下，KAN也能达到或超过使用大型MLP模型的准确性。在可解释性方面，通过监督和无监督的toy datasets以及数学和物理上的应用，展示了KAN可以帮助研究者们探索和发现科学任务上的规则和定律。

但是这些实验大多使用KAN去拟合固定的函数来分析其性质，并没有使用真实的下游任务来验证KAN的实际有效性。